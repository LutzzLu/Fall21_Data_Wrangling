---
title: "Genetics"
author: "Jiahui Li"
date: "11/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section{Introduction}

In order to solve the problem of computational complexity for our existing dataset, we use three dimension reduction methods, principle component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP) to reduce the dimension of data. After we get the dimension reduced datasets, we calculate quantitative measures and do density plots to have an overall understanding of the data, then, we visualize data by clustering. And then we finished the multi test. Based on our data analysis, we can do random forest to model these three datasets separately and compare the accuracy of the three models. According to the model accuracy comparison, we can find the corresponding method of dimensionality reduction that will be used for future new gene prediction. Finally, we take the data generated by PCA as an example for interaction visualization.

\section{Dimension Reduction}

After a general overview of the dataset, we found the data has no missing values and can be used directly for processing and analysis without the need for cleaning. So, the first step is solving the problem of computational complexity by reducing dimensionality.

Three dimension reduction methods, principle component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP) will be applied in this part.

The dimension reduction part will be started by listing the required packages.
```{r}
library(ggfortify)
library(factoextra)
library(FactoMineR)
library(tsne)
library(umap)
library(pROC)
library(randomForest)
library(ggbiplot)
```

Before applying the first method PCA, we read the data file and separate the quantitative data and labels, so that we can do dimension reduction only on quantitative data.
```{r}
data.pca = read.table('data.csv', sep = ',', header=TRUE)
label.pca = read.table('labels.csv', sep = ',', header=TRUE)
```
The data frame "data.pca" contains only quantitative data and "label.pca" contains labels that represent types of tumors.

Then we use function prcomp() in package "factoextra" to perform principal components analysis on the quantitative data frame "data.pca"
```{r}
com.pca <- prcomp(data.pca[,2:20532], center = TRUE)
```
We set center=TRUE to make the variables shift to be zero centered and get an object returned from this function named "com.pca". 

Next, we use function ggscreeplot() in package "ggbiplot" to do scree plot for principle components, and determine the number of principal components to keep in the PCA based on the plot.
```{r}
ggscreeplot(com.pca)
```
According to this scree test, the first and the second components are to the left of "elbow", where the eigenvalues seem to level off. So these two components should be retained as significant.

In the next step we can make the dimension reduced data frame and export a file by write.csv() function.
```{r}
test = com.pca$x
dr.pca = data.frame(class = label.pca$Class,
                    PC1 = test[, 1],
                    PC2 = test[, 2])
write.csv(dr.pca,file="pca_dimension_reduced.csv",quote=F,row.names = F)
```
In this step, we combine the quantitative data and labels together, and export a file nammed "dr_pca.csv" without quotes and row names.

After finishing PCA, we start to do t-SNE with separating the quantitative data and labels.
```{r}
data.tsne = read.table('data.csv', sep = ',', header=TRUE)
label.tsne = read.table('labels.csv', sep = ',', header=TRUE)
```
We obtained data frames "data.tsne" and "label.tsne" containing the quantitative data and labels, respectively.

Then we use the tsne() function to reduce "data.tsne" to two dimensional data by setting k=2. Since we guess the number of close neighbors each point has is 50, we set perplexity equal to 50 in the function.
```{r}
tsne.out = tsne(data.tsne[,2:20532],k=2,perplexity=50)
```

After we get the data after the dimensionality reduction by t-SNE, we combine the quantitative data and labels together and output file named "dr_tsne.csv"
```{r}
dr.tsne = data.frame(class = label.tsne$Class,
                     tsne1 = tsne.out[, 1],
                     tsne2 = tsne.out[, 2])
write.csv(dr.tsne,file="tsne_dimension_reduced.csv",quote=F,row.names = F)
```

Applying UMAP has a similar process to applying the first two methods, again we start with reading and separating the quantitative data and labels.
```{r}
data.umap = read.table('data.csv', sep = ',', header=TRUE)
label.umap = read.table('labels.csv', sep = ',', header=TRUE)
```

Then we take the data in the file "data.umap", except for the first column, and create a matrix by the as.matrix() function. After applying umap() function on the matrix, we combine the quantitative data and labels together and output file named "dr_umap.csv".
```{r}
data.umap.matrix = as.matrix(data.umap[,2:20532])
data.umap.output = umap(data.umap.matrix)
dr.umap = data.frame(class = label.umap$Class,
                     umap1 = data.umap.output$layout[, 1], 
                     umap2 = data.umap.output$layout[, 2])
write.csv(dr.umap,file="umap_dimension_reduced.csv",quote=F,row.names = F)
```

\section{Quantitative Analysis and Data Visualization}

For the three datasets we obtained by the three dimension reduction methods, we first get a general idea of the data by calculating the mean and variance of each column with different labels in each dataset. Also, we made density plots to assist in finding the data characteristics.

Let's start by reading the three datasets and setting column names
```{r}
pca=read.csv("pca_dimension_reduced.csv",header=TRUE,sep=",")
colnames(pca)=c("num","PCA_1","PCA_2","target")
tsne=read.csv("tsne_dimension_reduced.csv",header=TRUE,sep=",")
colnames(tsne)=c("num","tSNE_1","tSNE_2","target")
umap=read.csv("umap_dimension_reduced.csv",header=TRUE,sep=",")
colnames(umap)=c("num","UMAP_1","UMAP_2","target")
```

Then, we make a matrix for each dataset, the first column of the list is the label, followed by mean and variance of the corresponding label.
```{r}
T=unique(pca$target)

pca_mean1=c()
pca_mean2=c()
pca_var1=c()
pca_var2=c()
for(i in 1:5){
  pca_mean1[i]=mean(pca[which(pca$target==T[i]),]$PCA_1)
  pca_mean2[i]=mean(pca[which(pca$target==T[i]),]$PCA_2)
  pca_var1[i]=var(pca[which(pca$target==T[i]),]$PCA_1)
  pca_var2[i]=var(pca[which(pca$target==T[i]),]$PCA_2)
}
pcameasure=data.frame(T,pca_mean1,pca_mean2,pca_var1,pca_var2)
head(pcameasure)

tsne_mean1=c()
tsne_mean2=c()
tsne_var1=c()
tsne_var2=c()
for(i in 1:5){
  tsne_mean1[i]=mean(tsne[which(tsne$target==T[i]),]$tSNE_1)
  tsne_mean2[i]=mean(tsne[which(tsne$target==T[i]),]$tSNE_2)
  tsne_var1[i]=var(tsne[which(tsne$target==T[i]),]$tSNE_1)
  tsne_var2[i]=var(tsne[which(tsne$target==T[i]),]$tSNE_2)
}
tsnemeasure=data.frame(T,tsne_mean1,tsne_mean2,tsne_var1,tsne_var2)
head(tsnemeasure)

umap_mean1=c()
umap_mean2=c()
umap_var1=c()
umap_var2=c()
for(i in 1:5){
  umap_mean1[i]=mean(umap[which(tsne$target==T[i]),]$UMAP_1)
  umap_mean2[i]=mean(umap[which(tsne$target==T[i]),]$UMAP_2)
  umap_var1[i]=var(umap[which(tsne$target==T[i]),]$UMAP_1)
  umap_var2[i]=var(umap[which(tsne$target==T[i]),]$UMAP_2)
}
umapmeasure=data.frame(T,umap_mean1,umap_mean2,umap_var1,umap_var2)
head(umapmeasure)
```

Before we observe and compare the data characteristics of different labels in different datasets, we make density plots of the two columns for each dataset, with the first column plotted in black and the second in red.
```{r}
plot(density(pca$PCA_1))
lines(density(pca$PCA_2),col="red")
plot(density(tsne$tSNE_1))
lines(density(tsne$tSNE_2),col="red")
plot(density(umap$UMAP_1))
lines(density(umap$UMAP_2),col="red")
```
We make the following observations. 

\begin{enumerate}
\item The mean values of each of the two columns in the dataset obtained by dimensionality reduction with PCA are similar to those of t-SNE, however, the variance of each column in the PCA dataset is much graeter than that of t-SNE dataset.
\item The mean values of each of the two columns in the dataset obtained by dimensionality reduction with UMAP are not comparable to that of PCA and t-SNE datasets. The variance of each column in the UMAP dataset is much smaller than that of the other two dataset.
\item The density plot of UMAP has clear and narrow peaks and troughs, which corresponds to the feature of small variance. In contrast, the density plot of PCA has unclear and wide peaks, which corresponds to the feature of large variance. 
\end{enumerate}

After we finish observation of quantitative measurements, we visualize the dimensionality-reduced data by clustering based on labels. The features of data we observed before will be shown by clustering performance. In this part, the package "ggplot2" and function "ggplot" will be used.
```{r}
T=unique(pca$target)
tar_cols<-c("#de8f6e","#aac0aa","#2a1a1f","#bb0a21","#087e8b")
names(tar_cols)<-levels(pca$target)
ggplot(pca,aes(x=`PCA_1`,y=`PCA_2`,color=target))+
  geom_point()+ 
  labs(x="PCA_1",y="PCA_2")+
  scale_color_manual(values=tar_cols)

names(tar_cols)<-levels(tsne$target)
ggplot(tsne,aes(x=`tSNE_1`,y=`tSNE_2`,color=target))+
  geom_point()+ 
  labs(x="tSNE_1",y="tSNE_2")+
  scale_color_manual(values=tar_cols)

names(tar_cols)<-levels(umap$target)
ggplot(umap,aes(x=`UMAP_1`,y=`UMAP_2`,color=target))+
  geom_point()+
  labs(x="UMAP_1",y="UMAP_2")+
  scale_color_manual(values=tar_cols)
```
In addition to the ggplot() function, we use geom_point() function to create scatter plot, labs() to set axis, and function scale_color_manual() to change colors. We can find from the plots above that data from UMAP shows the best clustering performance, since the points of different labels are clustered together well and not mixed with each other, and many points with different labels mixed with each other in PCA. Dataset from t-SNE also perform well in clustering, however, there can be an outlier under label "LUAD" appears in "BRCA" point group.

\section{Comparison of Model Accuracy}

After we calculate the quantitative measures and visualize the data generated by each of the three dimension reduction methods, we select the most applicable dimensionality reduction method for our data by building a machine learning model. In this part, we apply the random forest function to each dataset and model it separately. Then, for each model, we change the number of trees to grow to find the smallest out-of-bag error rate. Next, we apply the predict() function to the model and get prediction matrix, which will be used to calculated accuracy. By comparing the accuracy of the three models, we can find the most applicable one that with the highest accuracy, and the corresponded dimension reduction method is the one we will use for predicting labels for new genes.

Let's start by installing package and reading files. 
```{r}
#install.packages("randomForest")
library(randomForest)
df.pca = read.csv('pca_dimension_reduced.csv', header = TRUE)
df.tsne = read.csv('tsne_dimension_reduced.csv', header = TRUE)
df.umap = read.csv('umap_dimension_reduced.csv', header = TRUE)
```

Then, we set training data and test data after letting the "target" columns be factors.
```{r}
df.pca$target = as.factor(df.pca$target)
ind.pca = sample(2, nrow(df.pca), replace = TRUE, prob = c(0.7, 0.3))
train.pca = df.pca[ind.pca == 1, ]
test.pca = df.pca[ind.pca == 2, ]

df.tsne$target = as.factor(df.tsne$target)
ind.tsne = sample(2, nrow(df.tsne), replace = TRUE, prob = c(0.7, 0.3))
train.tsne = df.tsne[ind.tsne == 1, ]
test.tsne = df.tsne[ind.tsne == 2, ]

df.umap$target = as.factor(df.umap$target)
ind.umap = sample(2, nrow(df.umap), replace = TRUE, prob = c(0.7, 0.3))
train.umap = df.umap[ind.umap == 1, ]
test.umap = df.umap[ind.umap == 2, ]
```

Next, we model the three training datasets separately with the parameter "mtry" equal to two since there are two variables in our dimension reduced data, and the parameter "ntree" be 1000. Then we print the model to check the out-of-bag error rate and plot the model to observe the fluctuations. 
```{r}
#set.seed(100)
ntree_fit.pca<-randomForest(target~.,data=train.pca,mtry=2,ntree=500)
print(ntree_fit.pca)
plot(ntree_fit.pca)

ntree_fit.tsne<-randomForest(target~.,data=train.tsne,mtry=2,ntree=1000)
print(ntree_fit.tsne)
plot(ntree_fit.tsne)

ntree_fit.umap<-randomForest(target~.,data=train.umap,mtry=2,ntree=1000)
print(ntree_fit.umap)
plot(ntree_fit.umap)
```
According to our observation of the plots, we found that fluctuations of the plot from PCA almost occur almost throughout the range 1000, so we set the number of trees to grow be 1000 in the model for predict. For models from t-SNE and UMAP, after trying different parameter values, we found 30 can be the smallest number of trees to keep the smallest out-of-bag error rate, since there are almost no fluctuations after 30.
```{r}
set.seed(100)

rf.pca = randomForest(target~., data = train.pca, mtry = 2, ntree = 1000)
rf.tsne = randomForest(target~., data = train.tsne, mtry = 2, ntree = 30)
rf.umap = randomForest(target~., data = train.umap, mtry = 2, ntree = 30)
```

For the next step, we use training data to predict the three models above by the predict() function and make matrix for each of them.
```{r}
pred.pca<-predict(rf.pca,data=train.pca)
Freq.pca<-table(pred.pca,train.pca$target)

pred.tsne<-predict(rf.tsne,data=train.tsne)
Freq.tsne<-table(pred.tsne,train.tsne$target)

pred.umap<-predict(rf.umap,data=train.umap)
Freq.umap<-table(pred.umap,train.umap$target)
```

Last, we calculate the accuracy for all three models by dividing the sum of the diagonal of the matrix we built before by the sum of the matrix.
```{r}
(accu.pca=sum(diag(Freq.pca))/sum(Freq.pca))
(accu.tsne=sum(diag(Freq.tsne))/sum(Freq.tsne))
(accu.umap=sum(diag(Freq.umap))/sum(Freq.umap))
```
Since the model from UMAP has the highest accuracy, we choose the dimension reduction method UMAP in our project for future label prediction of new genes.

\section{Interactive Visualization}

This below is the interactive plot for the PCA reduced Dimension plot. The shiny package is used to generate the interactive visualization. This interactive visualization allows the user to hover/click over a certain (x,y) coordinate on the graph in reads per kilo base of exon per million to identify the gene samples.

```{r}
# Loading the shiny package 
library(shiny)
library(ggplot2)
ui <- basicPage(
  titlePanel("Interactive PCA reduced Dimension Plot"),
  plotOutput("plot1",
             click = "plot_click",
             dblclick = "plot_dblclick",
             hover = "plot_hover",
             brush = "plot_brush"
  ),
  verbatimTextOutput("info")
)

server <- function(input, output) {
  pca=read.csv("pca_dimension_reduced.csv",header=TRUE,sep=",")
  colnames(pca)=c("num","PCA_1","PCA_2","target")
  T=unique(pca$target)
  tar_cols<-c("#de8f6e","#aac0aa","#2a1a1f","#bb0a21","#087e8b")
  names(tar_cols)<-levels(pca$target)
  ggpl1 <- ggplot(pca,aes(x=`PCA_1`,y=`PCA_2`,color=target))+
    geom_point()+ 
    labs(x="PCA_1",y="PCA_2")+
    scale_color_manual(values=tar_cols)+ 
    theme_classic()
  ggpl1
  output$plot1 <- renderPlot({ggpl1})
  
  output$info <- renderText({
    xy_str <- function(e) {
      if(is.null(e)) return("NULL\n")
      paste0("x=", round(e$x, 1), " y=", round(e$y, 1), "\n")
    }
    xy_range_str <- function(e) {
      if(is.null(e)) return("NULL\n")
      paste0("xmin=", round(e$xmin, 1), " xmax=", round(e$xmax, 1), 
             " ymin=", round(e$ymin, 1), " ymax=", round(e$ymax, 1))
    }
    
    paste0(
      "click: ", xy_str(input$plot_click),
      "dblclick: ", xy_str(input$plot_dblclick),
      "hover: ", xy_str(input$plot_hover),
      "brush: ", xy_range_str(input$plot_brush)
    )
  })
}

shinyApp(ui, server)
```

 
This below is the interactive plot for the Density plot for the PCA reduced. The shiny package is used to generate the interactive visualization.
```{r}
library(shiny)
library(ggplot2)
ui <- basicPage(
    plotOutput("plot1",
               click = "plot_click",
               dblclick = "plot_dblclick",
               hover = "plot_hover",
               brush = "plot_brush"
    ),
    verbatimTextOutput("info")
)

server <- function(input, output) {
    pca=read.csv("pca_dimension_reduced.csv",header=TRUE,sep=",")
    colnames(pca)=c("num","PCA_1","PCA_2","target")
    T=unique(pca$target)
    tar_cols<-c("#de8f6e","#aac0aa","#2a1a1f","#bb0a21","#087e8b")
    names(tar_cols)<-levels(pca$target)
    ggpl1 <- ggplot(pca,aes(x=`PCA_1`,y=`PCA_2`,color=target))+
        geom_point()+ 
        labs(x="PCA_1",y="PCA_2")+
        scale_color_manual(values=tar_cols)+ 
        theme_classic()
    ggpl1
    pca_mean1=c()
    pca_mean2=c()
    pca_var1=c()
    pca_var2=c()
    for(i in 1:5){
        pca_mean1[i]=mean(pca[which(pca$target==T[i]),]$PCA_1)
        pca_mean2[i]=mean(pca[which(pca$target==T[i]),]$PCA_2)
        pca_var1[i]=var(pca[which(pca$target==T[i]),]$PCA_1)
        pca_var2[i]=var(pca[which(pca$target==T[i]),]$PCA_2)
    }
    pcameasure=data.frame(T,pca_mean1,pca_mean2,pca_var1,pca_var2)
    
   
    
    output$plot1 <- renderPlot({plot(density(pca$PCA_1))
                                     lines(density(pca$PCA_2),col="red")})
    
    output$info <- renderText({
        xy_str <- function(e) {
            if(is.null(e)) return("NULL\n")
            paste0("x=", round(e$x, 1), " y=", round(e$y, 1), "\n")
        }
        xy_range_str <- function(e) {
            if(is.null(e)) return("NULL\n")
            paste0("xmin=", round(e$xmin, 1), " xmax=", round(e$xmax, 1), 
                   " ymin=", round(e$ymin, 1), " ymax=", round(e$ymax, 1))
        }
        
        paste0(
            "click: ", xy_str(input$plot_click),
            "dblclick: ", xy_str(input$plot_dblclick),
            "hover: ", xy_str(input$plot_hover),
            "brush: ", xy_range_str(input$plot_brush)
        )
    })
}

shinyApp(ui, server)
```

\section{Multiple Testing}

$m=20532$
$\mu_{i, B R C A}$ : mean gene expression value for gene $i$ among BRCA subjects.
- $\mu_{i, A L L}$ : mean gene expression value for gene $i$ among
- $\mathrm{ALL}$ subjects.
- For each gene $i, i=1, \ldots, m$ :
- $H_{0, i}: \mu_{i, B R C A}=\mu_{i, A L L}$
- $H_{A, i}: \mu_{i, B R C A} \neq \mu_{i, A L L}$
- Test each of the $m$ hypotheses using a two-sided t-test.

```{r}
#Multiple Testing
library(multtest)
library(data.table)

#setwd
setwd("~/Desktop/QBS Fall 21'/QBS 181/Final Project/TCGA-PANCAN-HiSeq-801x20531")

######file 1: data frame#######
dat <- fread("data.csv")

#change NA values to 0
dat[is.na(dat)] <- 0 

#check dat
dim(dat)
#View(dat)

######file 2: data labels####### 
lab <- fread("labels.csv")
#View(lab)

#remove first row
lab <- lab[-1, ]

#add the second column of the "label file" to dat
labels <- lab[, 2]
class(labels)
#labels <- as.vector(lab[, 2])
#lab[,2]
dat$labels <- labels
class(dat)

m = nrow(dat)
m
n = ncol(dat)
n
brca.sub = dat[which(dat$labels == "BRCA")]
brca.sub = select(brca.sub, -1)
brca.sub = select(brca.sub, -20532)
#data_new <- data[ , c("x1", "x3")]       
# Properly subset data

View(brca.sub)
class(brca.sub)
typeof(brca.sub)

all.sub = dat[which(dat$labels != "BRCA")]
all.sub = select(all.sub, -1)
all.sub = select(all.sub, -20532)

View(all.sub)
class(all.sub)
typeof(all.sub)


dat = select(dat, -1)
dat = select(dat, -20532)
View(dat)

#t-test
p.vals = apply(dat, 1, function(x) {
  return (t.test(x[unlist(brca.sub)], x[unlist(all.sub)],
                 alternative="two.sided")$p.value)})

hist(p.vals,breaks=40)

#to get the gene names
names(p.vals) = dat.gnames[, 2]
sort(p.vals, decreasing = F)
```