---
title: "Genetics"
author: "Jiahui Li"
date: "11/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section{Introduction}

\section{Dimension Reduction}

After a general overview of the dataset, we found the data has no missing values and can be used directly for processing and analysis without the need for cleaning. So, the first step is solving the problem of computational complexity by reducing dimensionality.

Three dimension reduction methods, principle component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE) and uniford manifold approximation and projection (UMAP) will be applied in this part.

The dimension reduction part will be started by listing the required packages.
```{r}
library(ggfortify)
library(factoextra)
library(FactoMineR)
library(tsne)
library(umap)
library(pROC)
library(randomForest)
library(ggbiplot)
```

Before applying the first method PCA, we read the data file and separate the quantitative data and labels, so that we can do dimension reduction only on quantitative data.
```{r}
data.pca = read.table('data.csv', sep = ',', header=TRUE)
label.pca = read.table('labels.csv', sep = ',', header=TRUE)
```
The data frame "data.pca" contains only quantitative data and "label.pca" contains labels that represent types of tumors.

Then we use function prcomp() in package "factoextra" to perform principal components analysis on the quantitative data frame "data.pca"
```{r}
com.pca <- prcomp(data.pca[,2:20532], center = TRUE)
```
We set center=TRUE to make the variables shift to be zero centered and get an object returned from this function named "com.pca". 

Next, we use function ggscreeplot() in package "ggbiplot" to do scree plot for principle components, and determine the number of principal components to keep in the PCA based on the plot.
```{r}
ggscreeplot(com.pca)
```
According to this scree test, the first and the second components are to the left of "elbow", where the eigenvalues seem to level off. So these two components should be retained as significant.

In the next step we can make the dimension reduced data frame and export a file by write.csv() function.
```{r}
test = com.pca$x############
dr.pca = data.frame(class = label.pca$Class,
                    PC1 = test[, 1],
                    PC2 = test[, 2])
write.csv(dr.pca,file="dr_pca.csv",quote=F,row.names = F)
```
In this step, we combine the quantitative data and labels together, and export a file nammed "dr_pca.csv" without quotes and row names.

After finishing PCA, we start to do t-SNE with separating the quantitative data and labels.
```{r}
data.tsne = read.table('data.csv', sep = ',', header=TRUE)
label.tsne = read.table('labels.csv', sep = ',', header=TRUE)
```
We obtained data frames "data.tsne" and "label.tsne" containing the quantitative data and labels, respectively.

Then we use the tsne() function to reduce "data.tsne" to two dimensional data by setting k=2. Since we guess the number of close neighbors each point has is 50, we set perplexity equal to 50 in the function.
```{r}
tsne.out = tsne(data.tsne[,2:20532],k=2,perplexity=50)
```

After we get the data after the dimensionality reduction by t-SNE, we combine the quantitative data and labels together and output file named "dr_tsne.csv"
```{r}
dr.tsne = data.frame(class = label.tsne$Class,
                     tsne1 = tsne.out[, 1],
                     tsne2 = tsne.out[, 2])
write.csv(dr.tsne,file="dr_tsne.csv",quote=F,row.names = F)
```

Applying UMAP has a similar process to applying the first two methods, again we start with reading and separating the quantitative data and labels.
```{r}
data.umap = read.table('data.csv', sep = ',', header=TRUE)
label.umap = read.table('labels.csv', sep = ',', header=TRUE)
```

Then we take the data in the file "data.umap", except for the first column, and create a matrix by the as.matrix() function. After applying umap() function on the matrix, we combine the quantitative data and labels together and output file named "dr_umap.csv".
```{r}
data.umap.matrix = as.matrix(data.umap[,2:20532])
data.umap.output = umap(data.umap.matrix)
dr.umap = data.frame(class = label.umap$Class,
                     umap1 = data.umap.output$layout[, 1], 
                     umap2 = data.umap.output$layout[, 2])
write.csv(dr.umap,file="dr_umap.csv",quote=F,row.names = F)
```

\section{Quantitative Analysis snd Data Visualization}

For the three datasets we obtained by the three dimension reduction methods, we first get a general idea of the data by calculating the mean and variance of each column with different labels in each dataset. Also, we made density plots to assist in finding the data characteristics.

Let's start by reading the three datasets and setting column names
```{r}
pca=read.csv("pca_dimension_reduced.csv",header=TRUE,sep=",")
colnames(pca)=c("num","PCA_1","PCA_2","target")
tsne=read.csv("tsne_dimension_reduced.csv",header=TRUE,sep=",")
colnames(tsne)=c("num","tSNE_1","tSNE_2","target")
umap=read.csv("umap_dimension_reduced.csv",header=TRUE,sep=",")
colnames(umap)=c("num","UMAP_1","UMAP_2","target")
```

Then, we make a matrix for each dataset, the first column of the list is the label, followed by mean and variance of the corresponding label.
```{r}
T=unique(pca$target)

pca_mean1=c()
pca_mean2=c()
pca_var1=c()
pca_var2=c()
for(i in 1:5){
  pca_mean1[i]=mean(pca[which(pca$target==T[i]),]$PCA_1)
  pca_mean2[i]=mean(pca[which(pca$target==T[i]),]$PCA_2)
  pca_var1[i]=var(pca[which(pca$target==T[i]),]$PCA_1)
  pca_var2[i]=var(pca[which(pca$target==T[i]),]$PCA_2)
}
pcameasure=data.frame(T,pca_mean1,pca_mean2,pca_var1,pca_var2)
head(pcameasure)

tsne_mean1=c()
tsne_mean2=c()
tsne_var1=c()
tsne_var2=c()
for(i in 1:5){
  tsne_mean1[i]=mean(tsne[which(tsne$target==T[i]),]$tSNE_1)
  tsne_mean2[i]=mean(tsne[which(tsne$target==T[i]),]$tSNE_2)
  tsne_var1[i]=var(tsne[which(tsne$target==T[i]),]$tSNE_1)
  tsne_var2[i]=var(tsne[which(tsne$target==T[i]),]$tSNE_2)
}
tsnemeasure=data.frame(T,tsne_mean1,tsne_mean2,tsne_var1,tsne_var2)
head(tsnemeasure)

umap_mean1=c()
umap_mean2=c()
umap_var1=c()
umap_var2=c()
for(i in 1:5){
  umap_mean1[i]=mean(umap[which(tsne$target==T[i]),]$UMAP_1)
  umap_mean2[i]=mean(umap[which(tsne$target==T[i]),]$UMAP_2)
  umap_var1[i]=var(umap[which(tsne$target==T[i]),]$UMAP_1)
  umap_var2[i]=var(umap[which(tsne$target==T[i]),]$UMAP_2)
}
umapmeasure=data.frame(T,umap_mean1,umap_mean2,umap_var1,umap_var2)
head(umapmeasure)
```

Before we observe and compare the data characteristics of different labels in different datasets, we make density plots of the two columns for each dataset, with the first column plotted in black and the second in red.
```{r}
plot(density(pca$PCA_1))
lines(density(pca$PCA_2),col="red")
plot(density(tsne$tSNE_1))
lines(density(tsne$tSNE_2),col="red")
plot(density(umap$UMAP_1))
lines(density(umap$UMAP_2),col="red")
```
We make the following observations. 

\begin{enumerate}
\item The mean values of each of the two columns in the dataset obtained by dimensionality reduction with PCA are similar to those of t-SNE, however, the variance of each column in the PCA dataset is much graeter than that of t-SNE dataset.
\item The mean values of each of the two columns in the dataset obtained by dimensionality reduction with UMAP are not comparable to that of PCA and t-SNE datasets. The variance of each column in the UMAP dataset is much smaller than that of the other two dataset.
\item The density plot of UMAP has clear and narrow peaks and troughs, which corresponds to the feature of small variance. In contrast, the density plot of PCA has unclear and wide peaks, which corresponds to the feature of large variance. 
\end{enumerate}

After we finish observation of quantitative measurements, we visualize the dimensionality-reduced data by clustering based on labels. The features of data we observed before will be shown by clustering performance. In this part, the package "ggplot2" and function "ggplot" will be used.
```{r}
T=unique(pca$target)
tar_cols<-c("#de8f6e","#aac0aa","#2a1a1f","#bb0a21","#087e8b")
names(tar_cols)<-levels(pca$target)
ggplot(pca,aes(x=`PCA_1`,y=`PCA_2`,color=target))+
  geom_point()+ 
  labs(x="PCA_1",y="PCA_2")+
  scale_color_manual(values=tar_cols)

names(tar_cols)<-levels(tsne$target)
ggplot(tsne,aes(x=`tSNE_1`,y=`tSNE_2`,color=target))+
  geom_point()+ 
  labs(x="tSNE_1",y="tSNE_2")+
  scale_color_manual(values=tar_cols)

names(tar_cols)<-levels(umap$target)
ggplot(umap,aes(x=`UMAP_1`,y=`UMAP_2`,color=target))+
  geom_point()+
  labs(x="UMAP_1",y="UMAP_2")+
  scale_color_manual(values=tar_cols)
```
In addition to the ggplot() function, we use geom_point() function to create scatter plot, labs() to set axis, and function scale_color_manual() to change colors. We can find from the plots above that data from UMAP shows the best clustering performance, since the points of different labels are clustered together well and not mixed with each other, and many points with different labels mixed with each other in PCA. Dataset from t-SNE also perform well in clustering, however, there can be an outlier under label "LUAD" appears in "BRCA" point group.

\section{Comparison of Model Accuracy}

After we calculate the quantitative measures and visualize the data generated by each of the three dimension reduction methods, we select the most applicable dimensionality reduction method for our data by building a machine learning model. In this part, we apply the random forest function to each dataset and model it separately. Then, for each model, we change the number of variables randomly sampled as candidates at each split and number of trees to grow to find the smallest out-of-bag error rate. Next, we apply the predict() function to the model and get prediction matrix, which will be used to calculated accuracy. By comparing the accuracy of the three models, we can find the most applicable one that with the highest accuracy, and the corresponded dimension reduction method is the one we will use for predicting labels for new genes.

Let's start by installing package and reading files. 
```{r}
library(randomForest)
df.pca = read.csv('dr_pca.csv', header = TRUE)
df.tsne = read.csv('dr_tsne.csv', header = TRUE)
df.umap = read.csv('dr_umap.csv', header = TRUE)
```

Then, we set training data and test data after letting the "class" columns be factors.
```{r}
df.pca$class = as.factor(df.pca$class)
ind.pca = sample(2, nrow(df.pca), replace = TRUE, prob = c(0.7, 0.3))
train.pca = df.pca[ind.pca == 1, ]
test.pca = df.pca[ind.pca == 2, ]

df.tsne$class = as.factor(df.tsne$class)
ind.tsne = sample(2, nrow(df.tsne), replace = TRUE, prob = c(0.7, 0.3))
train.tsne = df.tsne[ind.tsne == 1, ]
test.tsne = df.tsne[ind.tsne == 2, ]

df.umap$class = as.factor(df.umap$class)
ind.umap = sample(2, nrow(df.umap), replace = TRUE, prob = c(0.7, 0.3))
train.umap = df.umap[ind.umap == 1, ]
test.umap = df.umap[ind.umap == 2, ]
```

Next, we model the three training datasets separately and, after trying different parameter values, we found 2 variables randomly sampled as candidates at each split and 500 trees would get the smallest out-of-bag error rate for all three models.
```{r}
set.seed(100)
rf.pca = randomForest(class~., data = train.pca, mtry = 2, ntree = 500)
rf.tsne = randomForest(class~., data = train.tsne, mtry = 2, ntree = 500)
rf.umap = randomForest(class~., data = train.umap, mtry = 2, ntree = 500)
```

For the next step, we use test data to predict the three models above by the predict() function and make matrix for each of them.
```{r}
pred.pca<-predict(rf.pca,data=test.pca)#####
Freq.pca<-table(pred.pca,train.pca$class)

pred.tsne<-predict(rf.tsne,data=train.tsne)#####3
Freq.tsne<-table(pred.tsne,train.tsne$class)

pred.umap<-predict(rf.umap,data=train.umap)#######
Freq.umap<-table(pred.umap,train.umap$class)
```

Last, we calculate the accuracy for all three models by dividing the sum of the diagonal of the matrix we built before by the sum of the matrix.
```{r}
accu.pca=sum(diag(Freq.pca))/sum(Freq.pca)
accu.tsne=sum(diag(Freq.tsne))/sum(Freq.tsne)
accu.umap=sum(diag(Freq.umap))/sum(Freq.umap)
```












